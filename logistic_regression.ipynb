{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "female-edition",
   "metadata": {},
   "source": [
    "# Logistic regression and gradient descent implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lesbian-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "circular-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic function \n",
    "#x: data inputs\n",
    "#lamda: weights or coeficients\n",
    "def logistic_f(x,lamda):\n",
    "    row,col=np.shape(x)\n",
    "    extra_col=np.transpose(np.ones(row))\n",
    "    x= np.c_[extra_col,x]\n",
    "    yhat=np.dot(x,lamda)\n",
    "    prob=[]\n",
    "    for i in range(len(yhat)):\n",
    "        prob.append(1/(1+math.exp(-yhat[i])))\n",
    "    return prob\n",
    "    \n",
    "    \n",
    "#objective function\n",
    "def log_loss(x,lamda,y):\n",
    "    #y: labels\n",
    "    F=logistic_f(x,lamda)\n",
    "    loss=0\n",
    "    for i in range(len(y)):\n",
    "        l=math.log(y[i]*F[i]+(1-y[i])*(1-F[i]))\n",
    "        loss=loss+l\n",
    "    return -loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "enclosed-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-step function to calculate gradients for each weight=lamda\n",
    "def derivate(x,lamda,y,i):\n",
    "    #i: index of each weight \n",
    "    #derivative at each weight is the sum of the derivatives for all data points at the given weight\n",
    "    row,col=np.shape(x)\n",
    "    extra_col=np.transpose(np.ones(row))\n",
    "    x_tilda=np.c_[extra_col,x]\n",
    "    F=logistic_f(x,lamda)\n",
    "    derivate=0\n",
    "    for n in range(row):\n",
    "        numerator=(y[n]-(1-y[n]))*(F[n]*(1-F[n])*(x_tilda[n,i]))\n",
    "        denominator=y[n]*F[n]+(1-y[n])*(1-F[n])\n",
    "        derivate=derivate+(numerator/denominator)\n",
    "    return(-derivate)\n",
    "\n",
    "\n",
    "#function to calculate gradients for each weight\n",
    "def gradient(x,lamda,y):\n",
    "    grad_temp=[]\n",
    "    for i in range(len(lamda)):\n",
    "        grad_temp.append(derivate(x,lamda,y,i))\n",
    "    return (grad_temp)\n",
    "#function that optimize the weigths= lamda\n",
    "def optimization(x,lamda,y,T,eta):\n",
    "    #T: epochs\n",
    "    #eta:learning rate\n",
    "    for i in range(T):\n",
    "        grad=np.array(gradient(x,lamda,y))\n",
    "        lamda=np.array(lamda)-(eta*grad)\n",
    "    return lamda\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-documentation",
   "metadata": {},
   "source": [
    "### Applying logistic regression and gradient descent to auto data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "victorian-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the auto dataset and creating a new variable 'high'\n",
    "Auto= pd.read_csv('auto.data.txt', delim_whitespace=True, na_values='?')\n",
    "Auto.dropna(inplace=True)\n",
    "#creating a new column for high\n",
    "Auto['high'] = np.where(Auto['mpg']>=23,1,0)\n",
    "\n",
    "#creating dummy variables for origin\n",
    "Auto['origin2'] = np.where(Auto['origin']==2,1,0)\n",
    "Auto['origin3'] = np.where(Auto['origin']==3,1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stable-terrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalising the features\n",
    "scaler= StandardScaler()\n",
    "data_scaled=Auto.loc[:,('horsepower','weight','year','origin2','origin3')]\n",
    "data_scaled=scaler.fit_transform(data_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "proof-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "#naming my x and y variables and initializing parameters\n",
    "y=Auto.loc[:,'high'].to_numpy()\n",
    "x=data_scaled\n",
    "init_lamda=np.transpose(np.array([0.1,0.2,0,1,-1,0.1]))\n",
    "T=100\n",
    "eta=0.0001\n",
    "lamda=optimization(x,init_lamda,y,T,eta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "attended-excess",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset randomly into training an test set. \n",
    "#80% for training\n",
    "data=np.c_[x,y]\n",
    "x_train,x_test,y_train,y_test=train_test_split(data[:,:5],data[:,-1],test_size=0.2,random_state=228)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "indie-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a prediction function for predicting the labels \n",
    "def predict(x,lamda):\n",
    "    prob=logistic_f(x,lamda)\n",
    "    y_pred=[]\n",
    "    for p in range(len(prob)):\n",
    "        if prob[p]>0.5:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "    return y_pred\n",
    "\n",
    "#defining error rate function\n",
    "def error_rate(y,y_pred):\n",
    "    error_rate=0\n",
    "    for i in range(len(y_test)):\n",
    "        error=(y_test[i]-y_pred[i])**2\n",
    "        error_rate+=error\n",
    "    return (error_rate/len(y_test))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "permanent-preference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " for eta: 0.1 and T: 100\n",
      "Training Error Rate:\n",
      " 0.5189873417721519\n",
      "Test Error Rate:\n",
      " 0.08860759493670886\n",
      "\n",
      " for eta: 0.01 and T: 100\n",
      "Training Error Rate:\n",
      " 0.4810126582278481\n",
      "Test Error Rate:\n",
      " 0.06329113924050633\n",
      "\n",
      " for eta: 0.001 and T: 100\n",
      "Training Error Rate:\n",
      " 0.46835443037974683\n",
      "Test Error Rate:\n",
      " 0.05063291139240506\n",
      "\n",
      " for eta: 0.0001 and T: 100\n",
      "Training Error Rate:\n",
      " 0.4936708860759494\n",
      "Test Error Rate:\n",
      " 0.08860759493670886\n",
      "\n",
      " for eta: 1e-05 and T: 100\n",
      "Training Error Rate:\n",
      " 0.43037974683544306\n",
      "Test Error Rate:\n",
      " 0.21518987341772153\n",
      "\n",
      " for eta: 1e-06 and T: 100\n",
      "Training Error Rate:\n",
      " 0.5316455696202531\n",
      "Test Error Rate:\n",
      " 0.189873417721519\n",
      "\n",
      " for eta: 1e-07 and T: 100\n",
      "Training Error Rate:\n",
      " 0.46835443037974683\n",
      "Test Error Rate:\n",
      " 0.43037974683544306\n",
      "\n",
      " for eta: 1e-08 and T: 100\n",
      "Training Error Rate:\n",
      " 0.4936708860759494\n",
      "Test Error Rate:\n",
      " 0.5316455696202531\n",
      "\n",
      " for eta: 1e-09 and T: 100\n",
      "Training Error Rate:\n",
      " 0.43037974683544306\n",
      "Test Error Rate:\n",
      " 0.3037974683544304\n",
      "\n",
      " for eta: 1e-10 and T: 100\n",
      "Training Error Rate:\n",
      " 0.5822784810126582\n",
      "Test Error Rate:\n",
      " 0.7848101265822784\n"
     ]
    }
   ],
   "source": [
    "#trying different learning rates\n",
    "#finding the error rate of the trained model on the test set\n",
    "np.random.seed(228)\n",
    "eta=0.00001\n",
    "etas=[.1,.01,.001,.0001,.00001,.000001,.0000001,.00000001,.000000001,.0000000001]\n",
    "T=100\n",
    "error_rate_train=[]\n",
    "error_rate_test=[]\n",
    "\n",
    "for i in range(len(etas)):\n",
    "    row,col=np.shape(x_train)\n",
    "    lamda_init=np.random.uniform(low=-.7,high=.71, size=col+1)\n",
    "    lamda=optimization(x_train,lamda_init,y_train,T,etas[i])\n",
    "    y_train_pred=predict(x_train,lamda)\n",
    "    y_test_pred=predict(x_test,lamda)\n",
    "    training_ER=error_rate(y_train,y_train_pred)\n",
    "    test_ER=error_rate(y_test,y_test_pred)\n",
    "    error_rate_train.append(training_ER)\n",
    "    error_rate_test.append(test_ER)\n",
    "    print('\\n for eta: {} and T: {}'.format(etas[i],T))    \n",
    "    print('Training Error Rate:\\n {}'.format(training_ER))\n",
    "    print('Test Error Rate:\\n {}'.format(test_ER))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "universal-maine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#choosing learning rate that produces the smallest test error rate\n",
    "eta=etas[error_rate_test.index(min(error_rate_test))]\n",
    "eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "continental-jonathan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Error Rate</th>\n",
       "      <th>Test Error Rate</th>\n",
       "      <th>T</th>\n",
       "      <th>Eta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.518987</td>\n",
       "      <td>0.088608</td>\n",
       "      <td>100</td>\n",
       "      <td>1.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.481013</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>100</td>\n",
       "      <td>1.000000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.468354</td>\n",
       "      <td>0.050633</td>\n",
       "      <td>100</td>\n",
       "      <td>1.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.493671</td>\n",
       "      <td>0.088608</td>\n",
       "      <td>100</td>\n",
       "      <td>1.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.430380</td>\n",
       "      <td>0.215190</td>\n",
       "      <td>100</td>\n",
       "      <td>1.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.531646</td>\n",
       "      <td>0.189873</td>\n",
       "      <td>100</td>\n",
       "      <td>1.000000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.468354</td>\n",
       "      <td>0.430380</td>\n",
       "      <td>100</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.493671</td>\n",
       "      <td>0.531646</td>\n",
       "      <td>100</td>\n",
       "      <td>1.000000e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.430380</td>\n",
       "      <td>0.303797</td>\n",
       "      <td>100</td>\n",
       "      <td>1.000000e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.784810</td>\n",
       "      <td>100</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Training Error Rate  Test Error Rate    T           Eta\n",
       "0             0.518987         0.088608  100  1.000000e-01\n",
       "1             0.481013         0.063291  100  1.000000e-02\n",
       "2             0.468354         0.050633  100  1.000000e-03\n",
       "3             0.493671         0.088608  100  1.000000e-04\n",
       "4             0.430380         0.215190  100  1.000000e-05\n",
       "5             0.531646         0.189873  100  1.000000e-06\n",
       "6             0.468354         0.430380  100  1.000000e-07\n",
       "7             0.493671         0.531646  100  1.000000e-08\n",
       "8             0.430380         0.303797  100  1.000000e-09\n",
       "9             0.582278         0.784810  100  1.000000e-10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generating a table to compare test and train error rates\n",
    "T=[]\n",
    "t=100\n",
    "for i in range(len(etas)):\n",
    "    T.append(t)\n",
    "data_table=np.c_[np.array(error_rate_train),np.array(error_rate_test)]\n",
    "data_table=pd.DataFrame(data=data_table,columns=['Training Error Rate','Test Error Rate'])\n",
    "\n",
    "data_table['T']=T\n",
    "data_table['Eta']=etas\n",
    "\n",
    "data_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-stereo",
   "metadata": {},
   "source": [
    "Traying different stopping criterias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "monthly-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of iteration when objective change more than 1%:\n",
      " 109\n",
      "Values of lamdas after 109 iterations:\n",
      " [-0.45706198 -1.32031894 -1.80791477  1.02941585  0.44181584  0.58528332]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(228)\n",
    "eta=eta\n",
    "lamda=np.random.uniform(low=-.7,high=.71, size=col+1)\n",
    "num_iter=0\n",
    "\n",
    "while True:\n",
    "    grad=np.array(gradient(x_train,lamda,y_train))\n",
    "    lamda=np.array(lamda)-(eta*grad)\n",
    "    if num_iter%10==0:\n",
    "        initial_value=log_loss(x_train,lamda,y_train)\n",
    "       \n",
    "       \n",
    "    else:\n",
    "        if num_iter%10==9:\n",
    "            final_value=log_loss(x_train,lamda,y_train)\n",
    "            if ((initial_value-final_value)/initial_value)*100<=1:\n",
    "                lamda_opt=lamda\n",
    "                print('No of iteration when objective change more than 1%:\\n {}'.format(num_iter))\n",
    "                print('Values of lamdas after {} iterations:\\n {}'.format(num_iter,lamda_opt))\n",
    "                break\n",
    "    \n",
    "    num_iter=num_iter+1\n",
    " \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "durable-grade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error Rate:\n",
      " 0.46835443037974683\n",
      "Test Error Rate:\n",
      " 0.05063291139240506\n"
     ]
    }
   ],
   "source": [
    "#calculating error rate with optimal learning rate\n",
    "lamda=lamda_opt\n",
    "y_train_pred=predict(x_train,lamda)\n",
    "y_test_pred=predict(x_test,lamda)\n",
    "training_ER=error_rate(y_train,y_train_pred)\n",
    "test_ER=error_rate(y_test,y_test_pred)\n",
    "\n",
    "print('Training Error Rate:\\n {}'.format(training_ER))\n",
    "print('Test Error Rate:\\n {}'.format(test_ER))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-naples",
   "metadata": {},
   "source": [
    "plotting the test error rate for 100 different models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "union-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(228)\n",
    "eta\n",
    "T=100\n",
    "test_error_rate=[]\n",
    "for i in range(T):\n",
    "    lamda=np.random.uniform(low=-.7,high=.71, size=col+1)\n",
    "    lamda_opt=optimization(x_train,lamda,y_train,T,eta)\n",
    "    y_test_pred=predict(x_test,lamda_opt)\n",
    "    test_error_rate.append(error_rate(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "surgical-lottery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAANKklEQVR4nO3dYYhd+V2H8efbLEHQbsmSESST3QRJ1WCLq9coFLToLmQrJEJVEii4sDYIphZbxIglXeMrW6ivIjTIUhF2Y9wXMuJIEK0UZbfMXbuuJiF1iG0z8cVOd1P7Qmwa/fkid+t1cmfumeyZuZv/Ph8YuOecf+75BZZnT869d26qCknS/e8dsx5AktQPgy5JjTDoktQIgy5JjTDoktSIB2Z14t27d9e+fftmdXpJui+99NJLX6+quUnHZhb0ffv2MRwOZ3V6SbovJfnqese85SJJjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktSImX2wSNouSbblPH63gGbNoKt5mw1tEuOs+5K3XCSpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhrRKehJDie5mmQ5yakJxx9O8vkkX0rySpIP9D+qJGkjU4OeZAdwFngCOAgcT3JwzbJPABeq6lHgGPCHfQ8qSdpYlyv0Q8ByVV2rqlvAeeDomjUFPDh6/C7g3/sbUZLURZeg7wGuj22vjPaNexr4UJIVYBH4yKQnSnIiyTDJcHV19R7GlSStp68XRY8Dn6uqeeADwJ8kueu5q+pcVQ2qajA3N9fTqSVJ0C3oN4C9Y9vzo33jngIuAFTVC8B3Abv7GFCS1E2XoC8BB5LsT7KTOy96LqxZ8zXgZwGS/BB3gu49FUnaRlODXlW3gZPAReAKd97NcinJmSRHRss+Dnw4yT8BzwFPlr9QWpK2VacvuKiqRe682Dm+7/TY48vA+/odTZK0GX5SVJIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqRGdgp7kcJKrSZaTnJpw/A+SvDz6+XKSb/Q+qSRpQ1O/UzTJDuAs8DiwAiwlWRh9jygAVfUbY+s/Ajy6BbNKkjbQ5Qr9ELBcVdeq6hZwHji6wfrjwHN9DCdJ6q5L0PcA18e2V0b77pLkEWA/8LfrHD+RZJhkuLq6utlZJUkb6PtF0WPA81X135MOVtW5qhpU1WBubq7nU0vS21uXoN8A9o5tz4/2TXIMb7dI0kx0CfoScCDJ/iQ7uRPthbWLkvwgsAt4od8RJUldTA16Vd0GTgIXgSvAhaq6lORMkiNjS48B56uqtmZUSdJGpr5tEaCqFoHFNftOr9l+ur+xJEmb5SdFJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGtEp6EkOJ7maZDnJqXXW/FKSy0kuJXm23zElSdNM/U7RJDuAs8DjwAqwlGShqi6PrTkA/Dbwvqq6meR7t2pgSdJkXa7QDwHLVXWtqm4B54Gja9Z8GDhbVTcBqurVfseUJE3TJeh7gOtj2yujfePeDbw7yT8keTHJ4UlPlOREkmGS4erq6r1NLEmaaOotl008zwHg/cA88IUk76mqb4wvqqpzwDmAwWBQPZ1bbyMPPfQQN2/e3PLzJNnyc+zatYvXX399y8+jt48uQb8B7B3bnh/tG7cCfLGqvg38W5IvcyfwS71MKY3cvHmTqjauBbbjfxp6e+lyy2UJOJBkf5KdwDFgYc2aP+fO1TlJdnPnFsy1/saUJE0zNehVdRs4CVwErgAXqupSkjNJjoyWXQReS3IZ+Dzwm1X12lYNLUm6W2b1z9fBYFDD4XAm59b9K0lTt1xa+bto+yR5qaoGk475SVFJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGdAp6ksNJriZZTnJqwvEnk6wmeXn08yv9jypJ2sgD0xYk2QGcBR4HVoClJAtVdXnN0j+tqpNbMKMkqYMuV+iHgOWqulZVt4DzwNGtHUuStFldgr4HuD62vTLat9YHk7yS5Pkkeyc9UZITSYZJhqurq/cwriRpPX29KPoXwL6qei/w18AfT1pUVeeqalBVg7m5uZ5OLUmCbkG/AYxfcc+P9n1HVb1WVd8abf4R8GP9jCdJ6qpL0JeAA0n2J9kJHAMWxhck+b6xzSPAlf5GlCR1MfVdLlV1O8lJ4CKwA3imqi4lOQMMq2oB+PUkR4DbwOvAk1s4syRpglTVTE48GAxqOBzO5Ny6fyVhVv/N9q2lv4u2T5KXqmow6ZifFJWkRhh0SWqEQZekRhh0SWqEQZekRhh0SWrE1PehS28l9ckH4el3zXqMXtQnH5z1CGqMQdd9Jb/7zWbeu52EenrWU6gl3nKRpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqRKegJzmc5GqS5SSnNlj3wSSVZOLXI0mSts7UoCfZAZwFngAOAseTHJyw7p3AR4Ev9j2kJGm6Llfoh4DlqrpWVbeA88DRCet+D/h94L96nE+S1FGXoO8Bro9tr4z2fUeSHwX2VtVfbvRESU4kGSYZrq6ubnpYSdL63vSLokneAXwG+Pi0tVV1rqoGVTWYm5t7s6eWJI3pEvQbwN6x7fnRvje8E/hh4O+SfAX4SWDBF0YlaXt1CfoScCDJ/iQ7gWPAwhsHq+o/qmp3Ve2rqn3Ai8CRqhpuycSSpImmBr2qbgMngYvAFeBCVV1KcibJka0eUJLUTaevoKuqRWBxzb7T66x9/5sfS5K0WX5SVJIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqRGdgp7kcJKrSZaTnJpw/FeT/HOSl5P8fZKD/Y8qSdrI1KAn2QGcBZ4ADgLHJwT72ap6T1X9CPAp4DN9DypJ2liXK/RDwHJVXauqW8B54Oj4gqr65tjmdwPV34iSpC4e6LBmD3B9bHsF+Im1i5L8GvAxYCfwM5OeKMkJ4ATAww8/vNlZJUkb6O1F0ao6W1XfD/wW8Il11pyrqkFVDebm5vo6tSSJbkG/Aewd254f7VvPeeDn38RMkqR70CXoS8CBJPuT7ASOAQvjC5IcGNv8OeBf+xtRktTF1HvoVXU7yUngIrADeKaqLiU5AwyragE4meQx4NvATeCXt3JoSdLdurwoSlUtAotr9p0ee/zRnueSJG2SnxSVpEYYdElqhEGXpEZ0uocuvZUkmfUIvdi1a9esR1BjDLruK1Vb/1slkmzLeaS+ectFkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhrRKehJDie5mmQ5yakJxz+W5HKSV5L8TZJH+h9VkrSRqUFPsgM4CzwBHASOJzm4ZtmXgEFVvRd4HvhU34NKkjbW5Qr9ELBcVdeq6hZwHjg6vqCqPl9V/znafBGY73dMSdI0XYK+B7g+tr0y2reep4C/mnQgyYkkwyTD1dXV7lNKkqbq9UXRJB8CBsCnJx2vqnNVNaiqwdzcXJ+nlqS3vS5fQXcD2Du2PT/a9/8keQz4HeCnq+pb/YwnSeqqyxX6EnAgyf4kO4FjwML4giSPAp8FjlTVq/2PKUmaZmrQq+o2cBK4CFwBLlTVpSRnkhwZLfs08D3AnyV5OcnCOk8nSdoiXW65UFWLwOKafafHHj/W81ySpE3yk6KS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1IhO31gk3c+SbMufqapN/xmpT52u0JMcTnI1yXKSUxOO/1SSf0xyO8kv9D+mdO+qalt+pFmbGvQkO4CzwBPAQeB4koNrln0NeBJ4tu8BJUnddLnlcghYrqprAEnOA0eBy28sqKqvjI79zxbMKEnqoMstlz3A9bHtldG+TUtyIskwyXB1dfVenkKStI5tfZdLVZ2rqkFVDebm5rbz1JLUvC5BvwHsHdueH+2TJL2FdAn6EnAgyf4kO4FjwMLWjiVJ2qypQa+q28BJ4CJwBbhQVZeSnElyBCDJjydZAX4R+GySS1s5tCTpbp0+WFRVi8Dimn2nxx4vcedWjCRpRjKrD0QkWQW+OpOTSxvbDXx91kNI63ikqia+q2RmQZfeqpIMq2ow6zmkzfKXc0lSIwy6JDXCoEt3OzfrAaR74T10SWqEV+iS1AiDLkmNMOjSSJJnkrya5F9mPYt0Lwy69H8+Bxye9RDSvTLo0khVfQF4fdZzSPfKoEtSIwy6JDXCoEtSIwy6JDXCoEsjSZ4DXgB+IMlKkqdmPZO0GX70X5Ia4RW6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXifwEEC3Ify6/fBgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(test_error_rate)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "ebad247e5534c1ba5b14ab8cc72fe2532bc9dd980a9cee8d1951b26de076f558"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
